{"version":3,"sources":["../../src/api/sources.js"],"names":["concatBucketsAndCrawlers","buckets","crawlers","map","bucket","type","id","crawler","description","config","storage","api","use","ensureAuthenticated","get","req","res","next","email","extractEmailFromHeaders","indexName","getUserIndex","Promise","all","getCrawlersSettingsByIndexName","mongoDb","getBucketsByIndexName","then","status","json","catch"],"mappings":"07BAAA;AACA,uD;AACA,uC;;AAEA,IAAMA,2BAA2B,SAA3BA,wBAA2B,CAACC,OAAD,EAAUC,QAAV;;AAEtBD,YAAQE,GAAR,CAAY,uCAAgBC,MAAhB,IAAwBC,MAAM,QAA9B,KAAZ,CAFsB;AAGtBH,aAASC,GAAT,CAAa,2BAAY,EAAEG,IAAIC,QAAQD,EAAd,EAAkBE,aAAaD,QAAQC,WAAvC,EAAoDH,MAAM,SAA1D,EAAZ,EAAb,CAHsB,IAAjC,C;;;AAMe,gBAAyB,KAAtBI,MAAsB,QAAtBA,MAAsB,CAAdC,OAAc,QAAdA,OAAc;AACpC,QAAIC,MAAM,sBAAV;;AAEAA,QAAIC,GAAJ,CAAQ,sBAAYC,mBAAZ,CAAgCH,OAAhC,CAAR;;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA4BAC,QAAIG,GAAJ,CAAQ,GAAR,EAAa,UAACC,GAAD,EAAMC,GAAN,EAAWC,IAAX,EAAoB;AAC7B,YAAMC,QAAQ,sBAAYC,uBAAZ,CAAoCJ,GAApC,CAAd;AACA,YAAMK,YAAY,sBAAYC,YAAZ,CAAyBH,KAAzB,CAAlB;;AAEAI,gBAAQC,GAAR,CAAY;AACR,6BAAWC,8BAAX,CAA0Cd,QAAQe,OAAlD,EAA2DL,SAA3D,CADQ;AAER,6BAAWM,qBAAX,CAAiChB,QAAQe,OAAzC,EAAkDL,SAAlD,CAFQ,CAAZ;;AAIKO,YAJL,CAIU,iBAAyB,sCAAvBzB,QAAuB,YAAbD,OAAa;AAC3Be,gBAAIY,MAAJ,CAAW,GAAX,EAAgBC,IAAhB,CAAqB7B,yBAAyBC,OAAzB,EAAkCC,QAAlC,CAArB;AACH,SANL;AAOK4B,aAPL,CAOWb,IAPX;AAQH,KAZD;;AAcA,WAAON,GAAP;AACH,C","file":"sources.js","sourcesContent":["import { Router } from 'express'\nimport ErrorResponse from '../utils/ErrorResponse'\nimport { MongoProxy, AuthService, CryptoService } from '../services'\n\nconst concatBucketsAndCrawlers = (buckets, crawlers) =>\n    [\n        ...buckets.map(bucket => ({ ...bucket, type: 'bucket' })),\n        ...crawlers.map(crawler => ({ id: crawler.id, description: crawler.description, type: 'crawler' }))\n    ]\n\nexport default ({ config, storage }) => {\n    let api = Router()\n\n    api.use(AuthService.ensureAuthenticated(storage))\n    \n    /**     \n     * @api {get} api/sources/ Get Available Sources\n     * @apiDescription Get Available Sources (Crawlers Included)     \n     * @apiGroup Sources                \n     *  \n     * @apiHeader {String} ambar-email User email.\n     * @apiHeader {String} ambar-email-token User token.\n     * \n     * @apiSuccessExample {json}  HTTP/1.1 200 OK\n     *    [\n     *       {\n     *           \"id\": \"Default\",\n     *           \"description\": \"Automatically created on UI upload\",\n     *           \"type\": \"bucket\"\n     *       }, \n     *       {\n     *           \"id\": \"Books\",\n     *           \"description\": \"Books crawler\",\n     *           \"type\": \"crawler\"\n     *       },\n     *       {\n     *           \"id\": \"Dropbox\",\n     *           \"description\": \"Dropbox Crawler\",\n     *           \"type\": \"crawler\"\n     *       }\n     *   ]\n     * \n     */\n    api.get('/', (req, res, next) => {\n        const email = AuthService.extractEmailFromHeaders(req)\n        const indexName = AuthService.getUserIndex(email)\n\n        Promise.all([\n            MongoProxy.getCrawlersSettingsByIndexName(storage.mongoDb, indexName),\n            MongoProxy.getBucketsByIndexName(storage.mongoDb, indexName)\n        ])\n            .then(([crawlers, buckets]) => {\n                res.status(200).json(concatBucketsAndCrawlers(buckets, crawlers))\n            })\n            .catch(next)\n    })\n\n    return api\n}\n"]}